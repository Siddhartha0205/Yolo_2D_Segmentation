# Yolo_2D_Segmentation

This project demonstrates the capabilities of the YOLO model regarding instantaneous segmentation. YOLOv8m is trained on a custom [dataset](https://github.com/ayoolaolafenwa/PixelLib/releases). 

I have included the information on the various steps followed for:
<ul>
  <li>creating the segmentation points for ground truths and labels</li> 
  <li>preprocessing the image and label dataset</li>
  <li>training the YOLO model</li>
</ul>


<h2>Dataset Preparation</h2>

<p> The task considered for this project is to identify butterflies in the image and draw a segmentation map around the butterfly. The dataset contains various images of the butterflies with different orientations and varying scales. The figure below demonstrates an example of the image dataset used for creating segmentation points. </p>

<p float="left">
  <img src="Pictures/butterfly_image.png" width="300" />
</p>

<p>Python pip package provides a tool <b>Labelme</b> to create ground truths and associate a label for the image. It generates a JSON file storing locations for markings of segmentation map along with the label. </p>

<p>The figure below shows the command used to install the Python tool <i>labelme</i></p>

<p float="left">
  <img src="Pictures/Labelme_library_scrennshot.png" width="500" />
</p>

<p>Then appears to tool to generate the ground truth of the image dataset. Add the images by providing paths of the <b>input folder</b> and the <b>output folder</b> from the <b>File menu</b>. Draw the polygon masks around the object of interest, i.e., the butterfly in this case, using the option <b>Add polygon mask</b>. </p>

<p float="left">
  <img src="Pictures/Labelme_GUI.png" width="500" />
</p>

<p>Upon achieving a closed polygon, the user will be able to provide a label to the image.</p>

<p float="left">
  <img src="Pictures/labelme_labelled.PNG" width="500" />
</p>

<p>A JSON file is generated storing the information about label data and normalized co-ordinates of segmentation mask. The screenshot below provides a glimpse of the generated information.</p>

<p float="left">
  <img src="Pictures/segmented_json_file.PNG" width="300" />
</p>

<p>Please mind that generating ground truths is too time consuming and demands a lot of patience. However, the generated json files are not suitable to train the YOLO model. In order to convert these files, Python pip provides another tool named <b>labelme2yolo</b>. This tools generates text files as output making it suitable for training the YOLO model both for object detection tasks and segmentation tasks. The figure below shows the command to install labelme2yolo tool using pip. </p>

<p float="left">
  <img src="Pictures/labelme2yolo.PNG" width="500" />
</p>

<p>The text files that contain information of label and ground truth co-ordinates are generated by using the command:</p>

<p float="left">
  <img src="Pictures/labelme2yolo_command.PNG" width="500" />
</p>

<p>Please be aware that the argument <b>polygon</b> is necessary to generate text files for segmentation task. By default, the tools generates output suitable for object detection but not for segmentation task.</p>

This concludes the dataset preparation phase. Now, the YOLO model could be downloaded and trained with this custom dataset.

<h2>YOLO Training</h2>

<b>YOLOv8m-seg</b> model is used in this project for detecting the presence of butterflies and adding a segmentation mask over the detection.

The paths for training and validation images are updated using an yaml file.

The table below provides the information on the most important libraries installed to train the model

| Library  | Description |
| ------------- | ------------- |
| Ultralytics  | Ultralytics provides the instance to download the YOLO model and the arguments necessary to initiate the training |
| Pytorch  | Pytorch provides the backbone framework to support the YOLO training  |

Though Ultralytics install necessary pytorch libraries automatically, they do not provide necessary CUDA support to access GPU modules in the system.

Hence it is preferred to download pytorch library from their [homepage](https://pytorch.org/get-started/locally/). These libraries provide CUDA support, thus model can access GPU for faster training.

The table below provides the information on training parameters:

| Parameter  | Value |
| ------------- | ------------- |
| task  | segment  |
| mode  | train |
| epochs  | 100 |
| batch size | 8  |
| imgsz  | 640 |

This table shows the number of samples used for training and validation

| Mode  | Number of samples |
| ------------- | ------------- |
| Training  | 300  |
| Validation  | 100 |

Upon successful training, pretrained model files <i>best.pt</i> and <i>last.pt</i> are generated in <i>runs</i> folder with some other analytic metrics. We can evaluate the performance of the classifier using <i>best.pt</i> model.

<h2>YOLO Evaluation</h2>
